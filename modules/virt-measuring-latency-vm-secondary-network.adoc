// Module included in the following assemblies:
//
// * virt/logging_events_monitoring/virt-running-cluster-checkups.adoc

:_content-type: PROCEDURE
[id="virt-measuring-latency-vm-secondary-network_{context}"]
= Checking network connectivity and latency for virtual machines on a secondary network

As a cluster administrator, you use a predefined checkup to verify network connectivity and measure latency between virtual machines (VMs) that are attached to a secondary network interface.

To run a checkup for the first time, follow the steps in the procedure.

If you have previously run a checkup, skip to step 5 of the procedure because the steps to install the framework and enable permissions for the checkup are not required.

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You logged in to the cluster as a user with the `cluster-admin` role.
* The cluster has at least two worker nodes.
* The Multus Container Network Interface (CNI) plug-in is installed on the cluster.
* You configured a network attachment definition for a namespace.

.Procedure

. Create a configuration file that contains the resources to set up the framework. This includes a namespace and service account for the framework, and the `ClusterRole` and `ClusterRoleBinding` objects to define permissions for the service account.
+
.Example framework manifest file
[%collapsible]
====
[source,yaml]
----
---
apiVersion: v1
kind: Namespace
metadata:
  name: kiagnose
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kiagnose
  namespace: kiagnose
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kiagnose
rules:
  - apiGroups: [ "" ]
    resources: [ "configmaps" ]
    verbs:
      - get
      - list
      - create
      - update
      - patch
      - delete
  - apiGroups: [ "rbac.authorization.k8s.io" ]
    resources:
      - roles
      - rolebindings
    verbs:
      - get
      - list
      - create
      - delete
  - apiGroups: [ "batch" ]
    resources: [ "jobs" ]
    verbs:
      - get
      - list
      - create
      - delete
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kiagnose
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kiagnose
subjects:
  - kind: ServiceAccount
    name: kiagnose
    namespace: kiagnose
...
----
====

. Apply the framework manifest:
+
[source,terminal]
----
$ oc apply -f <framework_manifest>.yaml
----

. Create a manifest file that contains the `ServiceAccount`, `Role`, and `RoleBinding` objects with permissions that the checkup requires for cluster access:
+
.Example role manifest file
[%collapsible]
====
[source,yaml]
----
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vm-latency-checkup-sa
  namespace: <target_namespace> <1>
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubevirt-vm-latency-checker
  namespace: <target_namespace>
rules:
- apiGroups: ["kubevirt.io"]
  resources: ["virtualmachineinstances"]
  verbs: ["get", "create", "delete"]
- apiGroups: ["subresources.kubevirt.io"]
  resources: ["virtualmachineinstances/console"]
  verbs: ["get"]
- apiGroups: ["k8s.cni.cncf.io"]
  resources: ["network-attachment-definitions"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubevirt-vm-latency-checker
  namespace: <target_namespace>
subjects:
- kind: ServiceAccount
  name: vm-latency-checkup-sa
roleRef:
  kind: Role
  name: kubevirt-vm-latency-checker
  apiGroup: rbac.authorization.k8s.io
----
====
<1> Specify the namespace where the checkup is to be executed. This must be an existing namespace where the `NetworkAttachmentDefinition` object resides.

. Apply the checkup roles manifest:
+
[source,terminal]
----
$ oc apply -f <latency_roles>.yaml
----

. Create a `ConfigMap` manifest that contains the input parameters for the checkup. The config map provides the input for the framework to run the checkup and also stores the results of the checkup.
+
.Example input config map
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubevirt-vm-latency-checkup-config
  namespace: <target_namespace> <1>
data:
  spec.image: registry.redhat.io/container-native-virtualization/vm-network-latency-checkup:v4.12.0
  spec.timeout: 5m
  spec.serviceAccountName: vm-latency-checkup-sa
  spec.param.network_attachment_definition_namespace: <target_namespace> <2>
  spec.param.network_attachment_definition_name: "bridge-network" <3>
  spec.param.max_desired_latency_milliseconds: "10" <4>
  spec.param.sample_duration_seconds: "5" <5>
  spec.param.source_node: "worker1" <6>
  spec.param.target_node: "worker2" <7>
----
<1> The namespace where the checkup is to be executed. This must be an existing namespace where the `NetworkAttachmentDefinition` object resides.
<2> The namespace where the `NetworkAttachmentDefinition` object resides.
<3> The name of the `NetworkAttachmentDefinition` object.
<4> Optional: The maximum desired latency, in milliseconds, between the virtual machines. If the measured latency exceeds this value, the checkup fails.
<5> Optional: The duration of the latency check, in seconds.
<6> Optional: When specified, latency is measured from this node to the target node. If the source node is specified, the `spec.param.target_node` field cannot be empty.
<7> Optional: When specified, latency is measured from the source node to this node.

. Apply the config map manifest in the frameworkâ€™s namespace:
+
[source,terminal]
----
$ oc apply -f <latency_config_map>.yaml
----

. Create a `Job` object to run the checkup:
+
.Example job manifest
[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: kubevirt-vm-latency-checkup
  namespace: kiagnose
spec:
  backoffLimit: 0
  template:
    spec:
      serviceAccount: kiagnose
      restartPolicy: Never
      containers:
        - name: framework
          image: registry.redhat.io/container-native-virtualization/checkup-framework:v4.12.0
          env:
            - name: CONFIGMAP_NAMESPACE
              value: <target_namespace>
            - name: CONFIGMAP_NAME
              value: kubevirt-vm-latency-checkup-config
----

. Apply the `Job` manifest. The checkup uses the ping utility to verify connectivity and measure latency.
+
[source,terminal]
----
$ oc apply -f <latency_job>.yaml
----

. Wait for the job to complete:
+
[source,terminal]
----
$ oc wait job kubevirt-vm-latency-checkup -n kiagnose --for condition=complete --timeout 6m
----

. Review the results of the latency checkup by running the following command. If the maximum measured latency is greater than the value of the `spec.param.max_desired_latency_milliseconds` attribute, the checkup fails and returns an error.
+
[source,terminal]
----
$ oc get configmap kubevirt-vm-latency-checkup-config -n <target_namespace> -o yaml
----
+
.Example output config map (success)
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubevirt-vm-latency-checkup-config
  namespace: <target_namespace>
data:
  spec.image: registry.redhat.io/container-native-virtualization/vm-network-latency-checkup:v4.12.0
  spec.timeout: 5m
  spec.serviceAccountName: vm-latency-checkup-sa
  spec.param.network_attachment_definition_namespace: <target_namespace>
  spec.param.network_attachment_definition_name: "bridge-network"
  spec.param.max_desired_latency_milliseconds: "10"
  spec.param.sample_duration_seconds: "5"
  spec.param.source_node: "worker1"
  spec.param.target_node: "worker2"
  status.succeeded: "true"
  status.failureReason: ""
  status.completionTimestamp: "2022-01-01T09:00:00Z"
  status.startTimestamp: "2022-01-01T09:00:07Z"
  status.result.avgLatencyNanoSec: "177000"
  status.result.maxLatencyNanoSec: "244000" <1>
  status.result.measurementDurationSec: "5"
  status.result.minLatencyNanoSec: "135000"
  status.result.sourceNode: "worker1"
  status.result.targetNode: "worker2"
----
<1> The maximum measured latency in nanoseconds.

. Optional: To view the detailed job log in case of checkup failure, use the following command:
+
[source,terminal]
----
$ oc logs job.batch/kubevirt-vm-latency-checkup -n kiagnose
----

. Delete the job and config map resources that you previously created by running the following commands:
+
[source,terminal]
----
$ oc delete job -n kiagnose kubevirt-vm-latency-checkup
----
+
[source,terminal]
----
$ oc delete config-map -n <target_namespace> kubevirt-vm-latency-checkup-config
----

. Optional: If you do not plan to run another checkup, delete the checkup role and framework manifest files.
+
[source,terminal]
----
$ oc delete -f <file_name>.yaml
----
